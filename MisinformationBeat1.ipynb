{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9355fd90",
   "metadata": {},
   "source": [
    "# Misinformation Beat Notebook\n",
    "\n",
    "This file will be used to double check the results of our research on the Misinformation Beat. \n",
    "\n",
    "Since the main data was pulled via ProQuestTDM studio, the raw data at the XML level is not available to double check. '\n",
    "\n",
    "Therefore, this file should be used to check the logic of the code, but can only be ran locally after the ANALYSIS portion (Excluding imports which can run). \n",
    "\n",
    "Run each box in order \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f6514f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "109b35a8",
   "metadata": {},
   "source": [
    "==================================================================\n",
    "\n",
    "\"We used Proquest TDM Studio to access the ProQuest “U.S. Major Dailies” dataset which includes historical archives of The Chicago Tribune, Los Angeles Times, New York Times, Wall Street Journal, and Washington Post. We searched all articles that used the phrases “misinformation,” “disinformation,” “propaganda,” “conspiracy theory,” “conspiracy theories” and “fake news” from January 1, 1980 to April 24, 2025.The data received from ProQuest came in the form of XML files for each of the 231,992 articles that met the criteria.\"\n",
    "\n",
    "\n",
    "\n",
    "Step 1: Extract From XML Files\n",
    "All XML files resulting from query are stored in the data/*search term* folder (*INACCESSABLE*)\n",
    "Results of each extraction will be in the results/ folder\n",
    "\n",
    "Note: This cannot be recreated here given that the xml files are contained within the ProQuest TDM environment. This is meant for code review only. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f45f0",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd33d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with imports\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "import csv\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs('FinalTDMOutputs', exist_ok=True)\n",
    "os.makedirs('FullClean', exist_ok=True)\n",
    "os.makedirs('MatrixResults', exist_ok=True)\n",
    "os.makedirs('MatrixResults/Main', exist_ok=True)\n",
    "os.makedirs('MatrixResults/TermSearchMatrix', exist_ok=True)\n",
    "\n",
    "\n",
    "print('done with imports')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0114c1f",
   "metadata": {},
   "source": [
    "\"The data received from ProQuest came in the form of XML files for each of the 231,992 articles that met the criteria. To accurately parse the data, we removed any markup tags in order to ensure that only the readable text was captured. To clean the dataset and correct for any issues from text identification during scanning, we removed all non-alphanumeric and non-ASCII characters other than hyphens and ending punctuations; question marks and periods were replaced with spaces. Since hyphens are often attached to the keywords listed above, we removed hyphens that preceded or trailed a given keyword.\"\n",
    "\n",
    "===================================================================\n",
    "\n",
    "#Clean the text\n",
    "\n",
    "\n",
    "- Change keyword based on search as it defaults to 'disinformation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e28164a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with building clean function!\n"
     ]
    }
   ],
   "source": [
    "def clean_and_correct_text(text, keyword='disinformation'):\n",
    "    if not text or text.strip() == \"\":\n",
    "        return \"N/A\"\n",
    "\n",
    "    text = re.sub(r\"<.*?>\", \"\", text) #Removes markup text like <tag>\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s\\-\\.]+\", '', text)  # Keeps ASCII, periods and hyphens\n",
    "    text = text.replace('.', ' ') #Replace Periods with space\n",
    "    text = text.replace('?', ' ') #Replace Question marks with space\n",
    "    pattern = rf'(?<=\\b{re.escape(keyword)})-(?=\\w)|(?<=\\w)-(?=\\b{re.escape(keyword)})' #Remove hyphens from keyword (keep them elsewhere)\n",
    "    text = re.sub(pattern, ' ', text) #Apply regex\n",
    "\n",
    "    return text\n",
    "\n",
    "print('Done with building clean function!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b328fc",
   "metadata": {},
   "source": [
    "\"We then extracted every use of each of these search terms throughout all of the articles from each dataset. We included the 25 words that preceded and followed each instance of each search term to capture its context. Within a single article, multiple occurrences of each search term were counted separately.\"\n",
    "\n",
    "\n",
    "================================================\n",
    "# Extract Data\n",
    "\n",
    "This Function is used to extract data from individual xml files, and returns a list of objects that include GOID, Publisher, Title, Numeric Date, and the 50 surrounding words. \n",
    "#This function includes an implementation of the above clean and correct function. \n",
    "\n",
    "\n",
    "- Change keyword based on search as it defaults to 'disinformation'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e192e567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction and split functions ready as well!!!\n"
     ]
    }
   ],
   "source": [
    "# Function to extract metadata and concordances\n",
    "# Returns either FALSE or a list of concordances\n",
    "def extract_data_from_xml(file_path, keyword='disinformation', window=25):\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extract metadata fields\n",
    "        goid = root.find(\".//GOID\").text if root.find(\".//GOID\") is not None else \"N/A\"\n",
    "        publisher = root.find(\".//publisher/PublisherName\").text if root.find(\".//publisher/PublisherName\") is not None else \"N/A\"\n",
    "        title = root.find(\".//Title\").text if root.find(\".//Title\") is not None else \"N/A\"\n",
    "        numeric_date = root.find(\".//NumericDate\").text if root.find(\".//NumericDate\") is not None else \"N/A\"\n",
    "\n",
    "        # Extract article text\n",
    "        text_elem = root.find(\".//Text\")\n",
    "        full_text = text_elem.text if text_elem is not None and text_elem.text else \"N/A\"\n",
    "        \n",
    "        # If phrase is more than one word, capture its length\n",
    "        phrase_words = keyword.lower().split()\n",
    "        phrase_len = len(phrase_words)\n",
    "        \n",
    "        theConcordances = []\n",
    "\n",
    "        # Split text into words\n",
    "        words = clean_and_correct_text(full_text, keyword).split()\n",
    "        \n",
    "        # Find occurrences of the keyword and store concordances.\n",
    "        # Phrase words determine length of window\n",
    "        for i in range(len(words) - (phrase_len)): #For every word in the list (minus the length of the phrase)\n",
    "            window_slice = words[i:i + phrase_len] #look at number of words equal to the length of the phrase\n",
    "            if [w.lower() for w in window_slice] == phrase_words: #If the words in the window are equal to the phrase words\n",
    "                left_context = \" \".join(words[max(0, i - window): i])\n",
    "                right_context = \" \".join(words[i + phrase_len: min(len(words), i + phrase_len + window)]) #right context is blank + join word\n",
    "                \n",
    "                cleaned_left = clean_and_correct_text(left_context).lower().split()  #Split this into individual words\n",
    "                cleaned_left += [\"\"] * (25 - len(cleaned_left))\n",
    "                cleaned_right = clean_and_correct_text(right_context).lower().split() #Split this into individual words\n",
    "                cleaned_right += [\"\"] * (25 - len(cleaned_right))\n",
    "                \n",
    "                # Append extracted data\n",
    "                theConcordances.append({\n",
    "                    \"GOID\": goid,\n",
    "                    \"Publisher\": publisher.lower(),\n",
    "                    \"Title\": clean_and_correct_text(title).lower(),\n",
    "                    \"NumericDate\": numeric_date,\n",
    "                    \"Left Context1\": cleaned_left[0],\n",
    "                    \"Left Context2\": cleaned_left[1],\n",
    "                    \"Left Context3\": cleaned_left[2],\n",
    "                    \"Left Context4\": cleaned_left[3],\n",
    "                    \"Left Context5\": cleaned_left[4],\n",
    "                    \"Left Context6\": cleaned_left[5],\n",
    "                    \"Left Context7\": cleaned_left[6],\n",
    "                    \"Left Context8\": cleaned_left[7],\n",
    "                    \"Left Context9\": cleaned_left[8],\n",
    "                    \"Left Context10\": cleaned_left[9],\n",
    "                    \"Left Context11\": cleaned_left[10],\n",
    "                    \"Left Context12\": cleaned_left[11] ,\n",
    "                    \"Left Context13\": cleaned_left[12],\n",
    "                    \"Left Context14\": cleaned_left[13],\n",
    "                    \"Left Context15\": cleaned_left[14],\n",
    "                    \"Left Context16\": cleaned_left[15],\n",
    "                    \"Left Context17\": cleaned_left[16],\n",
    "                    \"Left Context18\": cleaned_left[17],\n",
    "                    \"Left Context19\": cleaned_left[18],\n",
    "                    \"Left Context20\": cleaned_left[19],\n",
    "                    \"Left Context21\": cleaned_left[20],\n",
    "                    \"Left Context22\": cleaned_left[21],\n",
    "                    \"Left Context23\": cleaned_left[22],\n",
    "                    \"Left Context24\": cleaned_left[23],\n",
    "                    \"Left Context25\": cleaned_left[24],\n",
    "                    \"Keyword\": keyword,\n",
    "                    \"Right Context1\": cleaned_right[0],\n",
    "                    \"Right Context2\": cleaned_right[1],\n",
    "                    \"Right Context3\": cleaned_right[2],\n",
    "                    \"Right Context4\": cleaned_right[3],\n",
    "                    \"Right Context5\": cleaned_right[4],\n",
    "                    \"Right Context6\": cleaned_right[5],\n",
    "                    \"Right Context7\": cleaned_right[6],\n",
    "                    \"Right Context8\": cleaned_right[7],\n",
    "                    \"Right Context9\": cleaned_right[8],\n",
    "                    \"Right Context10\": cleaned_right[9],\n",
    "                    \"Right Context11\": cleaned_right[10],\n",
    "                    \"Right Context12\": cleaned_right[11],\n",
    "                    \"Right Context13\": cleaned_right[12],\n",
    "                    \"Right Context14\": cleaned_right[13],\n",
    "                    \"Right Context15\": cleaned_right[14],\n",
    "                    \"Right Context16\": cleaned_right[15],\n",
    "                    \"Right Context17\": cleaned_right[16],\n",
    "                    \"Right Context18\": cleaned_right[17],\n",
    "                    \"Right Context19\": cleaned_right[18],\n",
    "                    \"Right Context20\": cleaned_right[19],\n",
    "                    \"Right Context21\": cleaned_right[20],\n",
    "                    \"Right Context22\": cleaned_right[21],\n",
    "                    \"Right Context23\": cleaned_right[22],\n",
    "                    \"Right Context24\": cleaned_right[23],\n",
    "                    \"Right Context25\": cleaned_right[24]\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return False\n",
    "    return theConcordances\n",
    "\n",
    "def append_csv(source_path, target_path):\n",
    "    with open(source_path, 'r', newline='') as src, open(target_path, 'a', newline='') as tgt:\n",
    "        reader = csv.reader(src)\n",
    "        writer = csv.writer(tgt)\n",
    "\n",
    "        next(reader)  # Skip header in source\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "print('extraction and split functions ready as well!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c307d2",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN \n",
    "#This is the main driver that utilizes the above functions. It loops through every xml file, ensures that every instance of the keyword is captured, then stores results as a csv. \n",
    "\n",
    "#It is problematic if there are several instances of \"Keyword not in text\" or \"keyword in text\" appear. \n",
    "#prints progress every 1000 files. \n",
    "\n",
    "#The result should be a folder called \"FinalTDMOutputs\" with the relevant CSVs corresponding to each search term. \n",
    "\n",
    "- change folder path and key word based on term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda0774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin MAIN\n",
      "0\n",
      "Extracted data saved to FinalTDMOutputs/Cleaned_Full_Win25_disinformation.csv\n",
      "0\n",
      "Extracted data saved to FinalTDMOutputs/Cleaned_Full_Win25_misinformation.csv\n",
      "0\n",
      "Extracted data saved to FinalTDMOutputs/Cleaned_Full_Win25_conspiracy_theory.csv\n",
      "0\n",
      "Extracted data added to FinalTDMOutputs//Cleaned_Full_Win25_conspiracy_theory.csv\n",
      "0\n",
      "Extracted data saved to FinalTDMOutputs/Cleaned_Full_Win25_propaganda.csv\n",
      "0\n",
      "Extracted data saved to FinalTDMOutputs/Cleaned_Full_Win25_fake_news.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the folder containing XML files\n",
    "folder_path = \"data/MisinformationTerms\"  # Adjust if needed\n",
    "keywords = ['disinformation', 'misinformation', 'conspiracy theory', 'conspiracy theories', 'propaganda', 'fake news']\n",
    "window = 25\n",
    "\n",
    "# List to store extracted data\n",
    "print('Begin MAIN')\n",
    "\n",
    "for word in keywords:\n",
    "    concordances = [] \n",
    "    articlesMinusTerm = 0\n",
    "    if os.path.exists(folder_path):\n",
    "        xml_files = [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "        #Use this for sampling\n",
    "        #sample_size = min(200, len(xml_files))\n",
    "        #sampled_files = random.sample(xml_files, sample_size)\n",
    "        print('begin loop')\n",
    "        \n",
    "        \n",
    "        for i, file_name in enumerate(xml_files): #Change function to take in sampled_files for sampling\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            concordance = extract_data_from_xml(file_path, word, window)\n",
    "        \n",
    "            #This is here to ensure that all articles and instances of misinformation are actually picked up. \"misinformation was hardcoded as keyword \n",
    "            if concordance == False: #if keyword NOT in the text - mainly error checking. \n",
    "                articlesMinusTerm += 1\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                text_elem = root.find(\".//Text\")\n",
    "                full_text = text_elem.text if text_elem is not None and text_elem.text else \"N/A\"\n",
    "                print('#################')\n",
    "                if(word in full_text):\n",
    "                    print(f'{word} is in the text')\n",
    "                else:\n",
    "                    print(f'{word} is NOT in the text')\n",
    "\n",
    "                full_text = text_elem.text if text_elem is not None and text_elem.text else \"N/A\"\n",
    "            else:\n",
    "                concordances = concordances + concordance\n",
    "                \n",
    "            if (i + 1) % 1000 == 0:  # Show progress every 100 files\n",
    "                print(f\"Processed {i+1} out of {len(xml_files)} files\")\n",
    "                #print('current articlesMinusTerm', articlesMinusTerm)\n",
    "\n",
    "    # Convert extracted data to DataFrame\n",
    "    print(len(concordances))\n",
    "    df_concordances = pd.DataFrame(concordances)\n",
    "\n",
    "    # Save to CSV        \n",
    "    filepath = 'FinalTDMOutputs/'\n",
    "    filename = 'Cleaned_Full_Win' + str(window) + '_' + word.replace(\" \", \"_\") + '.csv'\n",
    "        \n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    df_concordances.to_csv(f'{filepath}/{filename}', index=False)\n",
    "    \n",
    "    if word == \"conspiracy theories\":\n",
    "        append_csv(f'{filepath}/{filename}', f'{filepath}/Cleaned_Full_Win25_conspiracy_theory.csv')\n",
    "        os.remove(f'{filepath}/{filename}')\n",
    "        os.rename(f'{filepath}/Cleaned_Full_Win25_conspiracy_theory.csv', f'{filepath}/Cleaned_Full_Win25_conspiracy.csv')\n",
    "        print(f'Extracted data added to {filepath}/Cleaned_Full_Win25_conspiracy.csv')\n",
    "        \n",
    "    else: \n",
    "        print(f\"Extracted data saved to {filepath}{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c115ba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created zip archive: MisinfoBeatRawData.zip\n"
     ]
    }
   ],
   "source": [
    "def zip_selected_items(include_list, output_zip='archive.zip'):\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for item in include_list:\n",
    "            full_path = os.path.join(current_dir, item)\n",
    "\n",
    "            if not os.path.exists(full_path):\n",
    "                print(f\"Skipping: {item} (not found)\")\n",
    "                continue\n",
    "\n",
    "            if os.path.isfile(full_path):\n",
    "                # Single file\n",
    "                zipf.write(full_path, arcname=item)\n",
    "            elif os.path.isdir(full_path):\n",
    "                # Walk through directory\n",
    "                for root, _, files in os.walk(full_path):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        arcname = os.path.relpath(file_path, current_dir)\n",
    "                        zipf.write(file_path, arcname)\n",
    "\n",
    "    print(f\"Created zip archive: {output_zip}\")\n",
    "\n",
    "\n",
    "output_zipfile='MisinfoBeatRawData.zip'\n",
    "include_these = ['FinalTDMOutputs']\n",
    "zip_selected_items(include_these, output_zip=output_zipfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
