{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9355fd90",
   "metadata": {},
   "source": [
    "# Misinformation Beat Notebook\n",
    "\n",
    "This file will be used to double check the results of our research on the Misinformation Beat. \n",
    "\n",
    "Since the main data was pulled via ProQuestTDM studio, the raw data at the XML level is not available to double check. '\n",
    "\n",
    "Therefore, this file should be used to check the logic of the code, but can only be ran locally after the ANALYSIS portion (Excluding imports which can run). \n",
    "\n",
    "Run each box in order \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b35a8",
   "metadata": {},
   "source": [
    "==================================================================\n",
    "\n",
    "\"We used Proquest TDM Studio to access the ProQuest “U.S. Major Dailies” dataset which includes historical archives of The Chicago Tribune, Los Angeles Times, New York Times, Wall Street Journal, and Washington Post. We searched all articles that used the phrases “misinformation,” “disinformation,” “propaganda,” “conspiracy theory,” “conspiracy theories” and “fake news” from January 1, 1980 to April 24, 2025.The data received from ProQuest came in the form of XML files for each of the 231,992 articles that met the criteria.\"\n",
    "\n",
    "\n",
    "\n",
    "Step 1: Extract From XML Files\n",
    "All XML files resulting from query are stored in the data/*search term* folder (*INACCESSABLE*)\n",
    "Results of each extraction will be in the results/ folder\n",
    "\n",
    "Note: This cannot be recreated here given that the xml files are contained within the ProQuest TDM environment. This is meant for code review only. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f45f0",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.makedirs('FinalTDMOutputs', exist_ok=True)\n",
    "os.makedirs('FullClean', exist_ok=True)\n",
    "os.makedirs('MatrixResults', exist_ok=True)\n",
    "os.makedirs('MatrixResults/Main', exist_ok=True)\n",
    "os.makedirs('MatrixResults/TermSearchMatrix', exist_ok=True)\n",
    "\n",
    "\n",
    "print('done with imports')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0114c1f",
   "metadata": {},
   "source": [
    "\"The data received from ProQuest came in the form of XML files for each of the 231,992 articles that met the criteria. To accurately parse the data, we removed any markup tags in order to ensure that only the readable text was captured. To clean the dataset and correct for any issues from text identification during scanning, we removed all non-alphanumeric and non-ASCII characters other than hyphens and ending punctuations; question marks and periods were replaced with spaces. Since hyphens are often attached to the keywords listed above, we removed hyphens that preceded or trailed a given keyword.\"\n",
    "\n",
    "===================================================================\n",
    "\n",
    "#Clean the text\n",
    "\n",
    "\n",
    "- Change keyword based on search as it defaults to 'disinformation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_correct_text(text, keyword='disinformation'):\n",
    "    if not text or text.strip() == \"\":\n",
    "        return \"N/A\"\n",
    "\n",
    "    text = re.sub(r\"<.*?>\", \"\", text) #Removes markup text like <tag>\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s\\-\\.]+\", '', text)  # Keeps ASCII, periods and hyphens\n",
    "    text = text.replace('.', ' ') #Replace Periods with space\n",
    "    text = text.replace('?', ' ') #Replace Question marks with space\n",
    "    pattern = rf'(?<=\\b{re.escape(keyword)})-(?=\\w)|(?<=\\w)-(?=\\b{re.escape(keyword)})' #Remove hyphens from keyword (keep them elsewhere)\n",
    "    text = re.sub(pattern, ' ', text) #Apply regex\n",
    "\n",
    "    return text\n",
    "\n",
    "print('Done with building clean function!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b328fc",
   "metadata": {},
   "source": [
    "\"We then extracted every use of each of these search terms throughout all of the articles from each dataset. We included the 25 words that preceded and followed each instance of each search term to capture its context. Within a single article, multiple occurrences of each search term were counted separately.\"\n",
    "\n",
    "\n",
    "================================================\n",
    "# Extract Data\n",
    "\n",
    "This Function is used to extract data from individual xml files, and returns a list of objects that include GOID, Publisher, Title, Numeric Date, and the 50 surrounding words. \n",
    "#This function includes an implementation of the above clean and correct function. \n",
    "\n",
    "\n",
    "- Change keyword based on search as it defaults to 'disinformation'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e192e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract metadata and concordances\n",
    "# Returns either FALSE or a list of concordances\n",
    "def extract_data_from_xml(file_path, keyword='disinformation', window=25):\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extract metadata fields\n",
    "        goid = root.find(\".//GOID\").text if root.find(\".//GOID\") is not None else \"N/A\"\n",
    "        publisher = root.find(\".//publisher/PublisherName\").text if root.find(\".//publisher/PublisherName\") is not None else \"N/A\"\n",
    "        title = root.find(\".//Title\").text if root.find(\".//Title\") is not None else \"N/A\"\n",
    "        numeric_date = root.find(\".//NumericDate\").text if root.find(\".//NumericDate\") is not None else \"N/A\"\n",
    "\n",
    "        # Extract article text\n",
    "        text_elem = root.find(\".//Text\")\n",
    "        full_text = text_elem.text if text_elem is not None and text_elem.text else \"N/A\"\n",
    "        \n",
    "        # If phrase is more than one word, capture its length\n",
    "        phrase_words = keyword.lower().split()\n",
    "        phrase_len = len(phrase_words)\n",
    "        \n",
    "        theConcordances = []\n",
    "\n",
    "        # Split text into words\n",
    "        words = clean_and_correct_text(full_text, keyword).split()\n",
    "        \n",
    "        # Find occurrences of the keyword and store concordances.\n",
    "        # Phrase words determine length of window\n",
    "        for i in range(len(words) - (phrase_len)): #For every word in the list (minus the length of the phrase)\n",
    "            window_slice = words[i:i + phrase_len] #look at number of words equal to the length of the phrase\n",
    "            if [w.lower() for w in window_slice] == phrase_words: #If the words in the window are equal to the phrase words\n",
    "                left_context = \" \".join(words[max(0, i - window): i])\n",
    "                right_context = \" \".join(words[i + phrase_len: min(len(words), i + phrase_len + window)]) #right context is blank + join word\n",
    "                \n",
    "                cleaned_left = clean_and_correct_text(left_context).lower().split()  #Split this into individual words\n",
    "                cleaned_left += [\"\"] * (25 - len(cleaned_left))\n",
    "                cleaned_right = clean_and_correct_text(right_context).lower().split() #Split this into individual words\n",
    "                cleaned_right += [\"\"] * (25 - len(cleaned_right))\n",
    "                \n",
    "                # Append extracted data\n",
    "                theConcordances.append({\n",
    "                    \"GOID\": goid,\n",
    "                    \"Publisher\": publisher.lower(),\n",
    "                    \"Title\": clean_and_correct_text(title).lower(),\n",
    "                    \"NumericDate\": numeric_date,\n",
    "                    \"Left Context1\": cleaned_left[0],\n",
    "                    \"Left Context2\": cleaned_left[1],\n",
    "                    \"Left Context3\": cleaned_left[2],\n",
    "                    \"Left Context4\": cleaned_left[3],\n",
    "                    \"Left Context5\": cleaned_left[4],\n",
    "                    \"Left Context6\": cleaned_left[5],\n",
    "                    \"Left Context7\": cleaned_left[6],\n",
    "                    \"Left Context8\": cleaned_left[7],\n",
    "                    \"Left Context9\": cleaned_left[8],\n",
    "                    \"Left Context10\": cleaned_left[9],\n",
    "                    \"Left Context11\": cleaned_left[10],\n",
    "                    \"Left Context12\": cleaned_left[11] ,\n",
    "                    \"Left Context13\": cleaned_left[12],\n",
    "                    \"Left Context14\": cleaned_left[13],\n",
    "                    \"Left Context15\": cleaned_left[14],\n",
    "                    \"Left Context16\": cleaned_left[15],\n",
    "                    \"Left Context17\": cleaned_left[16],\n",
    "                    \"Left Context18\": cleaned_left[17],\n",
    "                    \"Left Context19\": cleaned_left[18],\n",
    "                    \"Left Context20\": cleaned_left[19],\n",
    "                    \"Left Context21\": cleaned_left[20],\n",
    "                    \"Left Context22\": cleaned_left[21],\n",
    "                    \"Left Context23\": cleaned_left[22],\n",
    "                    \"Left Context24\": cleaned_left[23],\n",
    "                    \"Left Context25\": cleaned_left[24],\n",
    "                    \"Keyword\": keyword,\n",
    "                    \"Right Context1\": cleaned_right[0],\n",
    "                    \"Right Context2\": cleaned_right[1],\n",
    "                    \"Right Context3\": cleaned_right[2],\n",
    "                    \"Right Context4\": cleaned_right[3],\n",
    "                    \"Right Context5\": cleaned_right[4],\n",
    "                    \"Right Context6\": cleaned_right[5],\n",
    "                    \"Right Context7\": cleaned_right[6],\n",
    "                    \"Right Context8\": cleaned_right[7],\n",
    "                    \"Right Context9\": cleaned_right[8],\n",
    "                    \"Right Context10\": cleaned_right[9],\n",
    "                    \"Right Context11\": cleaned_right[10],\n",
    "                    \"Right Context12\": cleaned_right[11],\n",
    "                    \"Right Context13\": cleaned_right[12],\n",
    "                    \"Right Context14\": cleaned_right[13],\n",
    "                    \"Right Context15\": cleaned_right[14],\n",
    "                    \"Right Context16\": cleaned_right[15],\n",
    "                    \"Right Context17\": cleaned_right[16],\n",
    "                    \"Right Context18\": cleaned_right[17],\n",
    "                    \"Right Context19\": cleaned_right[18],\n",
    "                    \"Right Context20\": cleaned_right[19],\n",
    "                    \"Right Context21\": cleaned_right[20],\n",
    "                    \"Right Context22\": cleaned_right[21],\n",
    "                    \"Right Context23\": cleaned_right[22],\n",
    "                    \"Right Context24\": cleaned_right[23],\n",
    "                    \"Right Context25\": cleaned_right[24]\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return False\n",
    "    return theConcordances\n",
    "\n",
    "def append_csv(source_path, target_path):\n",
    "    with open(source_path, 'r', newline='') as src, open(target_path, 'a', newline='') as tgt:\n",
    "        reader = csv.reader(src)\n",
    "        writer = csv.writer(tgt)\n",
    "\n",
    "        next(reader)  # Skip header in source\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "print('extraction and split functions ready as well!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c307d2",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN \n",
    "#This is the main driver that utilizes the above functions. It loops through every xml file, ensures that every instance of the keyword is captured, then stores results as a csv. \n",
    "\n",
    "#It is problematic if there are several instances of \"Keyword not in text\" or \"keyword in text\" appear. \n",
    "#prints progress every 1000 files. \n",
    "\n",
    "#The result should be a folder called \"FinalTDMOutputs\" with the relevant CSVs corresponding to each search term. \n",
    "\n",
    "- change folder path and key word based on term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing XML files\n",
    "folder_path = \"data/MisinformationTerms\"  # Adjust if needed\n",
    "keywords = ['disinformation', 'misinformation', 'conspiracy theory', 'conspiracy theories', 'propaganda', 'fake news']\n",
    "window = 25\n",
    "\n",
    "# List to store extracted data\n",
    "print('Begin MAIN')\n",
    "\n",
    "for word in keywords:\n",
    "    concordances = [] \n",
    "    articlesMinusTerm = 0\n",
    "    if os.path.exists(folder_path):\n",
    "        xml_files = [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "        #Use this for sampling\n",
    "        #sample_size = min(200, len(xml_files))\n",
    "        #sampled_files = random.sample(xml_files, sample_size)\n",
    "        print('begin loop')\n",
    "        \n",
    "        \n",
    "        for i, file_name in enumerate(xml_files): #Change function to take in sampled_files for sampling\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            concordance = extract_data_from_xml(file_path, word, window)\n",
    "        \n",
    "            #This is here to ensure that all articles and instances of misinformation are actually picked up. \"misinformation was hardcoded as keyword \n",
    "            if concordance == False: #if keyword NOT in the text - mainly error checking. \n",
    "                articlesMinusTerm += 1\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                text_elem = root.find(\".//Text\")\n",
    "                full_text = text_elem.text if text_elem is not None and text_elem.text else \"N/A\"\n",
    "                print('#################')\n",
    "                if(word in full_text):\n",
    "                    print(f'{word} is in the text')\n",
    "                else:\n",
    "                    print(f'{word} is NOT in the text')\n",
    "\n",
    "                full_text = text_elem.text if text_elem is not None and text_elem.text else \"N/A\"\n",
    "            else:\n",
    "                concordances = concordances + concordance\n",
    "                \n",
    "            if (i + 1) % 1000 == 0:  # Show progress every 100 files\n",
    "                print(f\"Processed {i+1} out of {len(xml_files)} files\")\n",
    "                #print('current articlesMinusTerm', articlesMinusTerm)\n",
    "\n",
    "    # Convert extracted data to DataFrame\n",
    "    print(len(concordances))\n",
    "    df_concordances = pd.DataFrame(concordances)\n",
    "\n",
    "    # Save to CSV        \n",
    "    filepath = 'FinalTDMOutputs/'\n",
    "    filename = 'Cleaned_Full_Win' + str(window) + '_' + word.replace(\" \", \"_\") + '.csv'\n",
    "        \n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    df_concordances.to_csv(f'{filepath}/{filename}', index=False)\n",
    "    \n",
    "    if word == \"conspiracy theories\":\n",
    "        append_csv(f'{filepath}/{filename}', f'{filepath}/Cleaned_Full_Win25_conspiracy_theory.csv')\n",
    "        print(f'Extracted data added to {filepath}/Cleaned_Full_Win25_conspiracy_theory.csv')\n",
    "    else: \n",
    "        print(f\"Extracted data saved to {filepath}{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b388cb4",
   "metadata": {},
   "source": [
    "From this we should have a folder called \"FinalTDMOutputs\" containing the csvs for each of the five terms. \n",
    "\n",
    "Now we move to step 2\n",
    "\n",
    "=====================================================\n",
    "\n",
    "# -----Analyis----- \n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fd19c",
   "metadata": {},
   "source": [
    "========================================================\n",
    "\n",
    "# Duplicate Removal\n",
    "\n",
    "\"We then removed any duplicate title/context combinations.\"\n",
    "\n",
    "#Remove duplicates for all the files\n",
    "#Outputs files into a folder called FullClean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_and_save(file_paths, output_folder=\"FullClean\"):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    report = {}\n",
    "\n",
    "    for path in file_paths:\n",
    "        df = pd.read_csv(path)\n",
    "        original_count = len(df)\n",
    "\n",
    "        # Normalize titles for comparison\n",
    "        if 'Title' in df.columns:\n",
    "            df['title_key'] = df['Title'].astype(str).str.strip().str.lower() #lowercase \n",
    "        else:\n",
    "            df['title_key'] = ''\n",
    "\n",
    "        # Concatenate all context columns into one string\n",
    "        context_cols = [col for col in df.columns if 'Context' in col]\n",
    "        df['context_key'] = df[context_cols].astype(str).apply(lambda x: ' '.join(x.dropna()), axis=1)\n",
    "\n",
    "        # Remove duplicates based on title and context content\n",
    "        df_cleaned = df.drop_duplicates(subset=['title_key', 'context_key'], keep='first') #use dataframe \n",
    "\n",
    "        # Drop helper columns\n",
    "        df_cleaned = df_cleaned.drop(columns=['title_key', 'context_key'])\n",
    "\n",
    "        # Save cleaned file\n",
    "        filename = os.path.basename(path)\n",
    "        cleaned_path = os.path.join(output_folder, filename)\n",
    "        df_cleaned.to_csv(cleaned_path, index=False)\n",
    "\n",
    "        # Record and report\n",
    "        cleaned_count = len(df_cleaned)\n",
    "        removed_count = original_count - cleaned_count\n",
    "        report[filename] = removed_count\n",
    "        print(f\" {removed_count} duplicates removed — saved to {cleaned_path}\")\n",
    "\n",
    "    return report\n",
    "\n",
    "file_list = glob.glob(\"FinalTDMOutputs/*.csv\")\n",
    "report = remove_duplicates_and_save(file_list)\n",
    "\n",
    "# Print summary if wanted. \n",
    "# print(\"\\n Duplicate Removal Report:\")\n",
    "# for file, removed in report.items():\n",
    "#     print(f\"{file}: {removed} duplicates removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263ab40",
   "metadata": {},
   "source": [
    "=======================================================================================\n",
    "\n",
    "\"We removed all multi-letter stopwords (e.g. “the” “and” “of”) using the Natural Language Toolkit package, and augmenting their stopwords with our own list developed during data exploration (See Appendix). We further cleaned the text using a set of stem mappings developed during exploration (i.e. turning ‘ukrainian’ to ‘ukraine’, ‘trumps’ to ‘trump’, or ‘coronavirus’ to ‘covid19’)(See Appendix). We also included a custom list of n-grams based on data exploration to better capture core concepts; (i.e., “fake news”, “white house”, “u k” or “u s s r”) (See Appendix). We then removed any remaining single-letter words that were not a part of an n-gram.\"\n",
    "\n",
    "\n",
    "# Hardcoded ngrams and term mappings\n",
    "\n",
    "These functions clean the text and apply ngrams in the process of analysis. They are used later in the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60016ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded term mappings (can be expanded as needed)\n",
    "TERM_MAPPINGS = {\n",
    "    \n",
    "    #Platforms\n",
    "    'facebooks' : 'facebook',\n",
    "    'twitters': 'twitter',\n",
    "    'tweet': 'twitter',\n",
    "    'tweeting': 'twitter',\n",
    "    'tweeted': 'twitter',\n",
    "    'Tiktoker': 'tiktok',\n",
    "    'youtuber': 'youtube',\n",
    "    'redditor': 'reddit',\n",
    "    'militarys': 'military',\n",
    "    'militaries': 'military',\n",
    "    \n",
    "    #Trump\n",
    "    'trumps': 'trump',\n",
    "    'presidents': 'president',\n",
    "    'presidential': 'president',\n",
    "    'republicans': 'republican',\n",
    "    \n",
    "     #Nations\n",
    "    'koreas': 'korea',\n",
    "    'korean': 'korea',\n",
    "    \n",
    "    'mexican': 'mexico',\n",
    "    'mexicans': 'mexico',\n",
    "  \n",
    "    'russias': 'russia',\n",
    "    'russian': 'russia',\n",
    "    'kremlin': 'russia',\n",
    "    'putins': 'putin',\n",
    "\n",
    "    'chinese': 'china',\n",
    "    'chinas': 'china',\n",
    "   \n",
    "    'ukraines': 'ukraine',\n",
    "    'ukrainian': 'ukraine',\n",
    "    'ukrainians': 'ukraine',\n",
    "    \n",
    "\n",
    "    'germanys': 'germany',\n",
    "    'german': 'germany',\n",
    "    \n",
    "    'israels': 'israel',\n",
    "    'israeli': 'israel',\n",
    "    'israelis': 'israel',\n",
    "    \n",
    "    'iranian': 'iran',\n",
    "    'irans': 'iran',\n",
    "    'iranians': 'iran',\n",
    "    \n",
    "    'turkeys': 'turkey',\n",
    "    'turkish': 'turkey',\n",
    "    \n",
    "    'french': 'france',\n",
    "    'frances': 'france',\n",
    "    \n",
    "    'indian': 'india',\n",
    "    'indias': 'india',\n",
    "     \n",
    "    'pakistani': 'pakistan',\n",
    "    'pakistans': 'pakistan',\n",
    "    \n",
    "    'european': 'europe',\n",
    "    'europes': 'europe',\n",
    "    #Other countries added as needed\n",
    "    \n",
    "    #America\n",
    "    'american': 'america',\n",
    "    'americas': 'america',\n",
    "    'americans': 'america',\n",
    "    \n",
    "    #COVID\n",
    "    'vaccines': 'vaccine',\n",
    "    'vaccination': 'vaccine',\n",
    "    'vaccinations': 'vaccine',\n",
    "    'vaccinated': 'vaccine',\n",
    "    'vaccinate': 'vaccine',\n",
    "    'coronavirus': 'covid19',\n",
    "    'covid': 'covid19',\n",
    "    \n",
    "    #People\n",
    "    'conways': 'conway',\n",
    "    'clintons': 'clinton',\n",
    "    'democrats': 'democrats',\n",
    "    \n",
    "    #Words\n",
    "    'campaigns': 'campaign',\n",
    "    'elections':'election',\n",
    "    'spreading': 'spread',\n",
    "    'spreads': 'spread',\n",
    "    'platforms': 'platform',\n",
    "    'officials': 'official',\n",
    "    'governments': 'government',\n",
    "    'users': 'user',\n",
    "    'uses': 'use',\n",
    "    'used': 'use',\n",
    "    'journalists': 'journalist',\n",
    "    'medias': 'media',\n",
    "    'years': 'year',\n",
    "    'wars': 'war',\n",
    "    'groups': 'group',\n",
    "    'terrorists': 'terrorist',\n",
    "    'posts': 'post',\n",
    "    'posting': 'post',\n",
    "    'posted': 'post',\n",
    "    'sites': 'site',\n",
    "    'claimed': 'claim',\n",
    "    'claims': 'claim',\n",
    "    'companies': 'company',\n",
    "    'companys': 'company',\n",
    "    'accounts': 'account',\n",
    "    'efforts': 'effort',\n",
    "    'speeches': 'speech',\n",
    "    'stories': 'story',\n",
    "    'outlets': 'outlet',\n",
    "    'including': 'include',\n",
    "    'inclusion': 'include', \n",
    "    'worlds': 'world',\n",
    "        \n",
    "}\n",
    "\n",
    "\n",
    "MANUAL_EXCLUDE = {'also', 'said', 'mr', 'us', 'would', 'one', 'people', 'like', 'many', 'could', '', '--', ' ', 'de', 'la'}\n",
    "# N-GRAMS\n",
    "BIGRAMS = {\n",
    "    ('u', 'k'): 'uk',\n",
    "    ('u', 'n'): 'un',\n",
    "    ('u', 's'): 'us',\n",
    "    ('e', 'u'): 'eu',\n",
    "    ('jan', '6'): 'jan 6',\n",
    "    ('fake', 'news'): 'fake news',\n",
    "    ('white', 'house'): 'white house',\n",
    "    ('social', 'media'): 'social media',\n",
    "    ('new', 'media'): 'new media',\n",
    "    ('islamic', 'state'): 'islamic state',\n",
    "    ('press', 'secretary'): 'press secretary',\n",
    "    ('donald', 'trump'): 'trump',\n",
    "    ('president', 'trump'): 'trump',\n",
    "    ('president', 'biden'): 'biden',\n",
    "    \n",
    "    ('alternative', 'facts'): 'alternative facts',\n",
    "    ('alternative', 'fact'): 'alternative facts',\n",
    "    ('conspiracy', 'theory'): 'conspiracy theory',\n",
    "    ('conspiracy', 'theories'): 'conspiracy theory',\n",
    "    ('kellyanne', 'conway'): 'conway',\n",
    "    ('sean', 'spicer'): 'spicer',\n",
    "    ('a', 'i'): 'artificial intelligence',\n",
    "    ('artificial', 'intelligence'): 'artificial intelligence',\n",
    "    ('state', 'sponsored'): 'state sponsored',\n",
    "    ('soviet', 'union'): 'ussr',\n",
    "    ('european', 'union'): 'europe',\n",
    "    \n",
    "    ('north', 'korea'): 'north korea',\n",
    "    ('south', 'korea'): 'south korea',\n",
    "    \n",
    "    ('hillary', 'clinton'): 'clinton',\n",
    "}\n",
    "TRIGRAMS = {\n",
    "    \n",
    "    ('u', 's', 'a'): 'usa',\n",
    "    ('f', 'b', 'i'): 'fbi',\n",
    "    ('c', 'i', 'a'): 'cia',\n",
    "    ('g', 'r', 'u'): 'gru',\n",
    "    ('k', 'g', 'b'): 'kgb',\n",
    "    ('u', 'f', 'o'): 'ufo',\n",
    "    ('u', 'a', 'p'): 'uap',\n",
    "    ('i', 'r', 'a'): 'ira',\n",
    "    ('internet', 'research', 'agency'): 'ira',\n",
    "    ('social', 'media', 'platform'): 'social media',\n",
    "}\n",
    "QUADGRAMS = {\n",
    "    ('u', 's', 's', 'r'): 'ussr',\n",
    "}\n",
    "\n",
    "#taking these single letters out of stopwords library. We need these\n",
    "fixed = stopwords.words('english')\n",
    "fixed.remove('s')\n",
    "fixed.remove('t')\n",
    "fixed.remove('i')\n",
    "fixed.remove('a')\n",
    "stop_words = set(fixed)\n",
    "stop_words.update(MANUAL_EXCLUDE)\n",
    "\n",
    "def apply_stemming_and_cleaning(word):\n",
    "    \"\"\"\n",
    "    Lowercases, applies term mappings, and skips stopwords.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    if word in TERM_MAPPINGS:\n",
    "        word = TERM_MAPPINGS[word]\n",
    "    if word in stop_words:\n",
    "        return 0\n",
    "    return word\n",
    "\n",
    "\n",
    " #applies existing ngrams to a row of words. \n",
    "\n",
    "def apply_ngrams(word_list):\n",
    "    \"\"\"\n",
    "    Scans a list of words and replaces matching bigrams, trigrams, quadgrams.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    result = []\n",
    "    while i < len(word_list):\n",
    "        \n",
    "        # Check quadgrams first\n",
    "        if i + 3 < len(word_list) and tuple(word_list[i:i+4]) in QUADGRAMS: #If there are more opportunities for QGs, and the next four letters form a quadgram\n",
    "            result.append(QUADGRAMS[tuple(word_list[i:i+4])]) #add the quadgram  to the main list\n",
    "            i += 4\n",
    "        # Then trigrams\n",
    "        elif i + 2 < len(word_list) and tuple(word_list[i:i+3]) in TRIGRAMS:\n",
    "            result.append(TRIGRAMS[tuple(word_list[i:i+3])])\n",
    "            i += 3\n",
    "        # Then bigrams\n",
    "        elif i + 1 < len(word_list) and tuple(word_list[i:i+2]) in BIGRAMS:\n",
    "            #print(\"found\", (BIGRAMS[tuple(word_list[i:i+2])]))\n",
    "            # if (BIGRAMS[tuple(word_list[i:i+2])] == 'fake news'):\n",
    "            #     print('Fake news FOUND')\n",
    "            result.append(BIGRAMS[tuple(word_list[i:i+2])])\n",
    "            i += 2\n",
    "        else:\n",
    "            if not (len(word_list[i]) == 1): #remove single letter words\n",
    "                result.append(word_list[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "    return result\n",
    "\n",
    "print('Stemming and mapping functions created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f200074",
   "metadata": {},
   "source": [
    "# Helper to deal with filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_filename(filepath, start, end, termName=None):\n",
    "       # Extract the filename without extension\n",
    "        file_name = os.path.basename(filepath)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "        # Remove 'Cleaned_Full' prefix if present\n",
    "        if base_name.startswith(\"Cleaned_Full\"):\n",
    "            base_name = base_name.replace(\"Cleaned_Full\", \"\").strip('_')\n",
    "        \n",
    "        if termName:\n",
    "            base_name = f'{termName}_{base_name}_Matrix_{start}_{end}.csv'\n",
    "        else:\n",
    "            base_name = f'{base_name}_Matrix_{start}_{end}.csv'\n",
    "        \n",
    "        return base_name\n",
    "#todo, create new ngrams automatically\n",
    "print('get filename function created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df6b7d",
   "metadata": {},
   "source": [
    "# Process Single CSV \n",
    "Function to process individual csv and save the result in a table of the top terms by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_csv(filepath, top_n=20, start_year=None, end_year=None):\n",
    "    WordCountsByYear = {}\n",
    "    #=== Step 1: Load CSV into DataFrame ===\n",
    "    df = pd.read_csv(filepath)\n",
    "    # === Step 2: Identify context columns ===\n",
    "    context_columns = [col for col in df.columns if \"Context\" in col]\n",
    "    # === Step 3: Loop through rows and check year consistency ===\n",
    "    with tqdm(total=len(df), desc=\"Processing rows\") as pbar:\n",
    "        for index, row in df.iterrows():\n",
    "            pbar.update(1)\n",
    "            numeric_date = row.get(\"NumericDate\")\n",
    "            # Skip rows without a valid date\n",
    "            try:\n",
    "                row_year = pd.to_datetime(numeric_date).year\n",
    "            except (ValueError, TypeError):\n",
    "                continue  # Skip invalid dates\n",
    "            \n",
    "            # Skip row with years out of bounds\n",
    "            if not (row_year >= start_year) and (row_year <= end_year): \n",
    "                continue  # Skip rows not matching the target year\n",
    "        \n",
    "        # === Step 4: While in row loop, Loop through context cells to extract words ===\n",
    "            row_words = []\n",
    "            raw_row = []\n",
    "            for col in context_columns:\n",
    "                cell_value = str(row[col]) if pd.notnull(row[col]) else \"\"\n",
    "                \n",
    "                # Clean word \n",
    "                raw_row.append(cell_value)\n",
    "                cleaned_word = apply_stemming_and_cleaning(cell_value)\n",
    "                if not cleaned_word == 0:\n",
    "                    row_words.append(cleaned_word)\n",
    "        # == Step 5: While in row loop, apply NGram Search    \n",
    "            filtered_words = apply_ngrams(row_words)\n",
    "        # == STep 6: While in row loop, Extract Word Counts For Year/word combo\n",
    "            for word in filtered_words:\n",
    "                #if word == 'u':\n",
    "                    #print(raw_row)\n",
    "                if row_year not in WordCountsByYear: #if new year in set, add new year dictionary to master dictionary for csv\n",
    "                    WordCountsByYear[row_year] = {}\n",
    "                if not word in WordCountsByYear[row_year]: # if word is not null and word not in the year/word combo, add a new one\n",
    "                    WordCountsByYear[row_year][word] = 1\n",
    "                else:\n",
    "                    WordCountsByYear[row_year][word] += 1 # or add to existing year/word combo\n",
    "\n",
    "\n",
    "        # == 6a \n",
    "          \n",
    "                if row_year not in GlobalMatrix: #if new year in set, add new year dictionary to master dictionary for csv\n",
    "                    GlobalMatrix[row_year] = {}\n",
    "                if not word in GlobalMatrix[row_year]: # if word is not null and word not in the year/word combo, add a new one\n",
    "                    GlobalMatrix[row_year][word] = 1\n",
    "                else:\n",
    "                    GlobalMatrix[row_year][word] += 1 # or add to existing year/word combo\n",
    "            \n",
    "        \n",
    "                if not word in GlobalMatrix[row_year]: # if word is not null and word not in the year/word combo, add a new one\n",
    "                    GlobalMatrix[row_year][word] = 1\n",
    "                else:\n",
    "                    GlobalMatrix[row_year][word] += 1 # or add to existing year/word combo\n",
    "    \n",
    "    # == 6b\n",
    "                if not word in TotalMatrix: #if dictionary hasn't been created yet\n",
    "                    TotalMatrix[word] = 1 #create initial count \n",
    "                else:\n",
    "                    TotalMatrix[word] += 1 #add to total count\n",
    "            \n",
    "                    \n",
    "        \n",
    "    # == Step 7: Reorganize to sort by highest ccount \n",
    "    rows = []  # Prepare list to collect top words per year\n",
    "    for year, word_dict in WordCountsByYear.items():\n",
    "        # Convert word-count pairs to a sorted list (by count descending)\n",
    "        sorted_words = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        \n",
    "        # Build a row: Year + Word 1 + Count 1 + Word 2 + Count 2 + ...\n",
    "        row = {'Year': year} #Every row will be a year\n",
    "        for i, (word, count) in enumerate(sorted_words, start=1):\n",
    "            row[f'Word {i}'] = word #Add word to Collumn name\n",
    "            row[f'Count {i}'] = count #Add count to column name\n",
    "        rows.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame(rows).sort_values(by='Year')\n",
    "    # Save\n",
    "\n",
    "    output_path = get_output_filename(filepath, start_year, end_year)\n",
    "    result_df.to_csv(f'MatrixResults/Main/{output_path}', index=False)\n",
    "\n",
    "    print(f\"DataFrame saved to {output_path}\")\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    # result_df.to_csv('top_20_words_by_year.csv', index=False)\n",
    "print('process csv function created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24c1ac",
   "metadata": {},
   "source": [
    "# Process all CSVs \n",
    "\n",
    "this function creates a matrix for every term, while creating a global matrix. They will be stored in the folder MatrixResults "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6125f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_csvs(input_folder='FullClean', top_n=20, start_year=None, end_year=None): #Create Individual Matrices For All \n",
    "    \"\"\"\n",
    "    Loops through all CSVs in the input folder and processes them to create a matrix for each term\n",
    "    \"\"\"\n",
    "    TotalWordCounts = {}\n",
    "    GlobalWordCountsByYear = {}\n",
    "    \n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            filepath = os.path.join(input_folder, file)\n",
    "            process_single_csv(filepath, top_n=top_n, start_year=start_year, end_year=end_year)\n",
    "    \n",
    "    #Do same logic as individual csvs\n",
    "    rows = []  # Prepare list to collect top words per year\n",
    "\n",
    "    #Everytime a CSV is processed, it adds word to a global matrix file\n",
    "    for year, word_dict in GlobalMatrix.items(): #Loop through all word/count pairs found overall \n",
    "        # Convert word-count pairs to a sorted list (by count descending)\n",
    "        sorted_words = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        \n",
    "        # Build a row for output csv: Year + Word 1 + Count 1 + Word 2 + Count 2 + ...\n",
    "        row = {'Year': year} #Every row will be a year\n",
    "        for i, (word, count) in enumerate(sorted_words, start=1):\n",
    "            row[f'Word {i}'] = word #Add word to Collumn name\n",
    "            row[f'Count {i}'] = count #Add count to column name\n",
    "        rows.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame(rows).sort_values(by='Year')\n",
    "    # Save\n",
    "    \n",
    "    word_counts_series = pd.Series(TotalMatrix).sort_values(ascending=False)\n",
    "    print(word_counts_series.head(30))\n",
    "          \n",
    "          \n",
    "    output_path = 'GlobalMatrix.csv'\n",
    "    result_df.to_csv(f'MatrixResults/Main/{output_path}', index=False)\n",
    "\n",
    "print('process all csvs function created')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6e8f5",
   "metadata": {},
   "source": [
    "# Word Search All Terms\n",
    "\n",
    "This function runs specific word searches on the dataset\n",
    "\n",
    "Results will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ffef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_search_all_csvs(input_folder, search_terms, top_n=20, start_year=None, end_year=None, termNames='NO_NAME'):\n",
    "    \n",
    "    print('Searching for terms: ' + str(search_terms))\n",
    "    TotalWordCounts = {}\n",
    "    GlobalWordCountsByYear = {}\n",
    "    \n",
    "    search_terms = [term.lower() for term in search_terms]\n",
    "    \n",
    "    for file in os.listdir(input_folder):\n",
    "        WordCountsByYear = {}\n",
    "        if file.endswith('.csv'):\n",
    "            filepath = os.path.join(input_folder, file)\n",
    "    \n",
    "            \n",
    "            #=== Step 1: Load CSV into DataFrame ===\n",
    "            df = pd.read_csv(filepath)\n",
    "            # === Step 2: Identify context columns ===\n",
    "            context_columns = [col for col in df.columns if \"Context\" in col]\n",
    "            # === Step 3: Loop through rows and check year consistency ===\n",
    "            #with tqdm(total=len(df), desc=\"Processing rows\") as pbar:\n",
    "            \n",
    "            with tqdm(total=len(df), desc=f'Searching {filepath}') as pbar:\n",
    "                for index, row in df.iterrows():\n",
    "                    pbar.update(1)\n",
    "                    numeric_date = row.get(\"NumericDate\")\n",
    "                    # Skip rows without a valid date\n",
    "                    try:\n",
    "                        row_year = pd.to_datetime(numeric_date).year\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue  # Skip invalid dates\n",
    "                    \n",
    "                    # Skip row with years out of bounds\n",
    "                    if not row_year in range(start_year, end_year + 1): #(row_year >= start_year) and (row_year <= end_year): \n",
    "                        #print(f'row_ {row_year}')\n",
    "                        continue  # Skip rows not matching the target year\n",
    "                    #else:\n",
    "                        #print(f'row_ {row_year} is later thann {start_year} and arlier than {end_year}')\n",
    "                # === Step 4: While in row loop, Loop through context cells ===\n",
    "                    row_words = []\n",
    "                    raw_row = []\n",
    "                    for col in context_columns:\n",
    "                        cell_value = str(row[col]) if pd.notnull(row[col]) else \"\"\n",
    "                        \n",
    "                        # Clean word \n",
    "                        raw_row.append(cell_value)\n",
    "                        cleaned_word = apply_stemming_and_cleaning(cell_value)\n",
    "                        if not cleaned_word == 0:\n",
    "                            row_words.append(cleaned_word)\n",
    "                # == Step 5: While in row loop, apply NGram Search    \n",
    "                    filtered_words = apply_ngrams(row_words)\n",
    "                # == STep 6: While in row loop, Extract Word Counts For Year/word combo\n",
    "                    for word in filtered_words:\n",
    "                        if word in search_terms: #If word is one of our search terms\n",
    "                            \n",
    "                            if not word in TotalWordCounts: #if dictionary hasn't been created yet\n",
    "                                TotalWordCounts[word] = 1 #create initial count \n",
    "                            else:\n",
    "                                TotalWordCounts[word] += 1 #add to total count\n",
    "                                \n",
    "                            \n",
    "                            if row_year not in WordCountsByYear: #if new year in set, add new year dictionary to master dictionary for csv\n",
    "                                WordCountsByYear[row_year] = {}\n",
    "                            if not word in WordCountsByYear[row_year]: # if word is not null and word not in the year/word combo, add a new one\n",
    "                                WordCountsByYear[row_year][word] = 1\n",
    "                            else:\n",
    "                                WordCountsByYear[row_year][word] += 1 # or add to existing year/word combo\n",
    "                    \n",
    "                    \n",
    "                    for word in filtered_words:\n",
    "                        if word in search_terms: #If word is one of our search terms\n",
    "                            if row_year not in GlobalWordCountsByYear: #if new year in set, add new year dictionary to master dictionary for csv\n",
    "                                GlobalWordCountsByYear[row_year] = {}\n",
    "                            if not word in GlobalWordCountsByYear[row_year]: # if word is not null and word not in the year/word combo, add a new one\n",
    "                                GlobalWordCountsByYear[row_year][word] = 1\n",
    "                            else:\n",
    "                                GlobalWordCountsByYear[row_year][word] += 1 # or add to existing year/word combo\n",
    "                            \n",
    "                    \n",
    "            # == Step 7: Reorganize to sort by highest ccount \n",
    "            rows = []  # Prepare list to collect top words per year\n",
    "            \n",
    "            for year, word_dict in WordCountsByYear.items():\n",
    "                # Convert word-count pairs to a sorted list (by count descending)\n",
    "                sorted_words = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "                \n",
    "                # Build a row: Year + Word 1 + Count 1 + Word 2 + Count 2 + ...\n",
    "                row = {'Year': year} #Every row will be a year\n",
    "                for i, (word, count) in enumerate(sorted_words, start=1):\n",
    "                    row[f'Word {i}'] = word #Add word to Collumn name\n",
    "                    row[f'Count {i}'] = count #Add count to column name\n",
    "                rows.append(row)\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            result_df = pd.DataFrame(rows).sort_values(by='Year')\n",
    "            # Save\n",
    "\n",
    "            output_path = get_output_filename(filepath, start_year, end_year, termName=termNames)\n",
    "            result_df.to_csv(f'MatrixResults/TermSearchMatrix/{termNames}/{output_path}', index=False)\n",
    "\n",
    "            print(f\"DataFrame saved to {output_path}\")\n",
    "            \n",
    "        \n",
    "            \n",
    "    \n",
    "    word_counts_series = pd.Series(TotalWordCounts).sort_values(ascending=False)\n",
    "    print(word_counts_series.head(30))    \n",
    "    \n",
    "    rows = [] #cleanse\n",
    "    #Get Totals\n",
    "    for year, word_dict in GlobalWordCountsByYear.items():\n",
    "        # Convert word-count pairs to a sorted list (by count descending)\n",
    "        sorted_words = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        \n",
    "        # Build a row: Year + Word 1 + Count 1 + Word 2 + Count 2 + ...\n",
    "        row = {'Year': year} #Every row will be a year\n",
    "        for i, (word, count) in enumerate(sorted_words, start=1):\n",
    "            row[f'Word {i}'] = word #Add word to Collumn name\n",
    "            row[f'Count {i}'] = count #Add count to column name\n",
    "        rows.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame(rows).sort_values(by='Year')\n",
    "    # Save\n",
    "    \n",
    "\n",
    "    output_path = f'MatrixResults/TermSearchMatrix/{termNames}/{termNames}_GlobalMatrixTerms_{start_year}_{end_year}.csv'\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    \n",
    "    # result_df.to_csv('top_20_words_by_year.csv', index=False)\n",
    "\n",
    "print('word search function created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1895785",
   "metadata": {},
   "source": [
    "# MAIN DRIVERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f71bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates empty variables to be added\n",
    "GlobalMatrix = {}\n",
    "TotalMatrix = {}\n",
    "\n",
    "# Define folder paths\n",
    "INPUT_FOLDER = 'FullClean'\n",
    "\n",
    "TOP_N_WORDS = 30\n",
    "START_YEAR = 2016\n",
    "END_YEAR = 2024\n",
    "    \n",
    "countries_list = ['Afghanistan', 'ussr', 'yugoslavia', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Cook Islands', 'Costa Rica', \"Côte d'Ivoire\", 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', \"North korea\", 'South Korea', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'russia', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan, Province of China', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'uk', 'us', 'United States Minor Outlying Islands', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Vietnam', 'Virgin Islands', 'Wallis and Futuna', 'Yemen', 'Zambia', 'Zimbabwe']\n",
    "platforms_list = ['facebook', 'twitter', 'instagram', 'tiktok', 'youtube', 'snapchat', 'whatsapp', 'telegram', 'reddit']\n",
    "world_leaders = ['biden', 'trump', 'clinton', 'xi', 'putin', 'netanyahu']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91d15e",
   "metadata": {},
   "source": [
    "This does a full word search and creates tables for the various terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_csvs(top_n=TOP_N_WORDS, start_year=START_YEAR, end_year=END_YEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7483d5",
   "metadata": {},
   "source": [
    "This goes through each CSV and makes a matrix for each term based on the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 'platforms_list' and 'platform' with relevnat list\n",
    "word_search_all_csvs( INPUT_FOLDER, platforms_list, TOP_N_WORDS, START_YEAR, END_YEAR, 'platform') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
